{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca5aa3a-54d6-4774-9399-f6ed5743d9ca",
      "metadata": {
        "id": "2ca5aa3a-54d6-4774-9399-f6ed5743d9ca"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb29f19-3d83-4a2d-8518-cb0dc78e339b",
      "metadata": {
        "id": "0bb29f19-3d83-4a2d-8518-cb0dc78e339b"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163214e0-dc6e-49a2-b6ea-4c700f415acc",
      "metadata": {
        "id": "163214e0-dc6e-49a2-b6ea-4c700f415acc",
        "outputId": "30dc0c89-42a1-4095-b394-4be5c7a73e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY = '####'\n",
        "os.environ[\"GOOGLE_API_KEY\"]=GOOGLE_API_KEY\n",
        "\n",
        "genai.configure(api_key= GOOGLE_API_KEY)\n",
        "\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5506df-0ce3-4db4-8442-cfbdf63a75c3",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "5e5506df-0ce3-4db4-8442-cfbdf63a75c3",
        "outputId": "9e156847-c99a-48e8-b026-26aa926aa16d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Langchain with Gemini\n",
        "# Reference : https://python.langchain.com/docs/integrations/chat/google_generative_ai\n",
        "%pip install --upgrade --quiet  langchain-google-genai pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "997b4d43-64ad-47a4-b7d0-c3a6ef19e1d9",
      "metadata": {
        "id": "997b4d43-64ad-47a4-b7d0-c3a6ef19e1d9",
        "outputId": "4d670d44-11fb-4b39-b9b6-86ca8ce3cbac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/riddhi/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Authentication with LLM\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
        "output = model(\n",
        "    [\n",
        "        SystemMessage(content=\"Answer in details.\"),\n",
        "        HumanMessage(content=\"Who is Amazon's CEO?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305cf45b-a021-46e4-a2ad-29c5918055f7",
      "metadata": {
        "id": "305cf45b-a021-46e4-a2ad-29c5918055f7",
        "outputId": "096a5b1a-2436-4c1a-a497-644f4df73fa4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='**Andy Jassy**\\n\\n* Current CEO of Amazon\\n* Previously served as CEO of Amazon Web Services (AWS)\\n* Joined Amazon in 1997 as a marketing manager\\n* Held various leadership positions within the company, including Senior Vice President of Global Operations\\n* Succeeded Jeff Bezos as CEO in July 2021\\n\\n**Key Accomplishments as Amazon CEO:**\\n\\n* Oversaw Amazon\\'s expansion into new markets, such as healthcare and robotics\\n* Implemented initiatives to improve employee experience and reduce workplace injuries\\n* Focused on sustainability and environmental initiatives\\n* Led Amazon\\'s acquisition of MGM Studios, expanding the company\\'s presence in entertainment\\n\\n**Background and Education:**\\n\\n* Born in Scarsdale, New York\\n* Graduated from Harvard College with a Bachelor of Arts in English\\n* Received an MBA from Harvard Business School\\n* Prior to Amazon, worked as a management consultant at McKinsey & Company\\n\\n**Leadership Style:**\\n\\n* Known for his analytical and data-driven approach\\n* Values customer obsession and innovation\\n* Emphasizes employee empowerment and collaboration\\n* Believes in a \"working backwards\" philosophy, where teams start with the customer\\'s needs\\n\\n**Personal Interests:**\\n\\n* Passionate about music and plays guitar\\n* Enjoys sports, including running and cycling\\n* Supports various philanthropic organizations', response_metadata={'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af96a368-9b78-45eb-b361-957f57e3fc0b",
      "metadata": {
        "id": "af96a368-9b78-45eb-b361-957f57e3fc0b",
        "outputId": "f9ec5e70-cdba-48ca-ca36-d5b81c1bcf8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Andy Jassy**\n",
            "\n",
            "* Current CEO of Amazon\n",
            "* Previously served as CEO of Amazon Web Services (AWS)\n",
            "* Joined Amazon in 1997 as a marketing manager\n",
            "* Held various leadership positions within the company, including Senior Vice President of Global Operations\n",
            "* Succeeded Jeff Bezos as CEO in July 2021\n",
            "\n",
            "**Key Accomplishments as Amazon CEO:**\n",
            "\n",
            "* Oversaw Amazon's expansion into new markets, such as healthcare and robotics\n",
            "* Implemented initiatives to improve employee experience and reduce workplace injuries\n",
            "* Focused on sustainability and environmental initiatives\n",
            "* Led Amazon's acquisition of MGM Studios, expanding the company's presence in entertainment\n",
            "\n",
            "**Background and Education:**\n",
            "\n",
            "* Born in Scarsdale, New York\n",
            "* Graduated from Harvard College with a Bachelor of Arts in English\n",
            "* Received an MBA from Harvard Business School\n",
            "* Prior to Amazon, worked as a management consultant at McKinsey & Company\n",
            "\n",
            "**Leadership Style:**\n",
            "\n",
            "* Known for his analytical and data-driven approach\n",
            "* Values customer obsession and innovation\n",
            "* Emphasizes employee empowerment and collaboration\n",
            "* Believes in a \"working backwards\" philosophy, where teams start with the customer's needs\n",
            "\n",
            "**Personal Interests:**\n",
            "\n",
            "* Passionate about music and plays guitar\n",
            "* Enjoys sports, including running and cycling\n",
            "* Supports various philanthropic organizations\n"
          ]
        }
      ],
      "source": [
        "print(output.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ab3924-a7ae-458a-b6ea-10e73b669d8e",
      "metadata": {
        "id": "23ab3924-a7ae-458a-b6ea-10e73b669d8e",
        "outputId": "42748c08-0ddb-4d7c-e06f-f700f4275f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "import sys\n",
            "from operator import add\n",
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 2:\n",
            "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonWordCount\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "    counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
            "                  .map(lambda x: (x, 1)) \\\n",
            "                  .reduceByKey(add)\n",
            "    output = counts.collect()\n",
            "    for (word, count) in output:\n",
            "        print(\"%s: %i\" % (word, count))\n",
            "\n",
            "    spark.stop()\n",
            "\n",
            "This code is a Spark application that counts the number of occurrences of each word in a text file. It uses the SparkSession API to create a SparkSession object, which is the entry point to programming Spark applications. The code then reads the text file into a DataFrame, and uses the `flatMap` and `map` transformations to split the text into words and create key-value pairs where the key is the word and the value is 1. The `reduceByKey` transformation is then used to sum the values for each key, resulting in a DataFrame with the word counts. Finally, the `collect` action is used to collect the results into a list, which is then printed to the console.\n",
            "\n",
            "Here is a more detailed explanation of the code:\n",
            "\n",
            "* **Creating a SparkSession:**\n",
            "```\n",
            "spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonWordCount\")\\\n",
            "        .getOrCreate()\n",
            "```\n",
            "\n",
            "This code creates a SparkSession object, which is the entry point to programming Spark applications. The `appName` parameter is used to specify the name of the application, which will be displayed in the Spark UI.\n",
            "\n",
            "* **Reading the text file:**\n",
            "```\n",
            "lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "```\n",
            "\n",
            "This code reads the text file specified by the first argument to the script into a DataFrame. The `rdd` method is then used to convert the DataFrame into a Resilient Distributed Dataset (RDD), which is a distributed collection of data that can be processed in parallel. The `map` transformation is then used to extract the text from each row of the RDD.\n",
            "\n",
            "* **Splitting the text into words and creating key-value pairs:**\n",
            "```\n",
            "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
            "                  .map(lambda x: (x, 1))\n",
            "```\n",
            "\n",
            "This code uses the `flatMap` transformation to split the text in each row of the RDD into a sequence of words. The `map` transformation is then used to create a key-value pair for each word, where the key is the word and the value is 1.\n",
            "\n",
            "* **Summing the values for each key:**\n",
            "```\n",
            "counts = counts.reduceByKey(add)\n",
            "```\n",
            "\n",
            "This code uses the `reduceByKey` transformation to sum the values for each key in the RDD. The `add` function is used as the reducer, which adds the values for each key together.\n",
            "\n",
            "* **Collecting the results:**\n",
            "```\n",
            "output = counts.collect()\n",
            "```\n",
            "\n",
            "This code uses the `collect` action to collect the results of the `reduceByKey` transformation into a list.\n",
            "\n",
            "* **Printing the word counts:**\n",
            "```\n",
            "for (word, count) in output:\n",
            "        print(\"%s: %i\" % (word, count))\n",
            "```\n",
            "\n",
            "This code iterates over the list of word counts and prints each word and its count to the console.\n",
            "\n",
            "* **Stopping the SparkSession:**\n",
            "```\n",
            "spark.stop()\n",
            "```\n",
            "\n",
            "This code stops the SparkSession, which releases any resources that were allocated by the session.\n",
            "\n",
            "Thank you for using our service. If you have any further questions, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# This is 2.2 Analyse Spark code from Apache spark git repo\n",
        "import re\n",
        "\n",
        "# Function to interact with LLM: Authentication\n",
        "def query_llm(prompt):\n",
        "    # Replace with the LLM's API endpoint and your access token\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
        "    output = model(\n",
        "        [\n",
        "            SystemMessage(content=\"Answer in details\"),\n",
        "            HumanMessage(content=prompt)\n",
        "        ]\n",
        "    )\n",
        "    return output.content\n",
        "\n",
        "# Function to handle user queries\n",
        "def answer_query(query, file_path, directory_path):\n",
        "    # Preprocess the query\n",
        "    preprocessed_query = preprocess(query)\n",
        "\n",
        "    code_content = file_read(file_path)\n",
        "    #code_content = read_py_files(directory_path)\n",
        "\n",
        "    print(code_content)\n",
        "\n",
        "    # Use the LLM to answer the query about the Spark code\n",
        "    llm_answer = query_llm(preprocessed_query + code_content)\n",
        "\n",
        "    # Post-process the LLM's answer\n",
        "    processed_answer = postprocess(llm_answer)\n",
        "\n",
        "    return processed_answer\n",
        "\n",
        "def preprocess(query):\n",
        "    # Remove special characters and extra spaces\n",
        "    preprocessed_query = re.sub(r'[^\\w\\s]', '', query)\n",
        "    preprocessed_query = re.sub(r'\\s+', ' ', preprocessed_query)\n",
        "\n",
        "    return preprocessed_query.strip()\n",
        "\n",
        "\n",
        "def postprocess(llm_answer):\n",
        "    # Add a closing statement to the answer\n",
        "    filtered_answer = llm_answer + \"\\n\\nThank you for using our service. If you have any further questions, feel free to ask!\"\n",
        "\n",
        "    return filtered_answer\n",
        "\n",
        "# For single particular file\n",
        "def file_read(file_path):\n",
        "    # This method reads the py file from local and convert into\n",
        "    # string format without any formatting\n",
        "    with open(file_path, 'r') as file:\n",
        "        file_content = file.read()\n",
        "    return file_content\n",
        "\n",
        "\n",
        "# TODO: would be do it multiple files under the same repository\n",
        "# I have created this function to read multiple files within same repo\n",
        "def read_py_files(file_paths):\n",
        "    # This method reads multiple py files from local and\n",
        "    # concatenates their contents into a single string\n",
        "    all_contents = \"\"\n",
        "    for file_path in file_paths:\n",
        "        # Check if the file has a .py extension\n",
        "        if file_path.endswith('.py'):\n",
        "            with open(file_path, 'r') as file:\n",
        "                file_content = file.read()\n",
        "                all_contents += file_content\n",
        "    return all_contents\n",
        "\n",
        "# Example usage of answer_query function\n",
        "user_query = \"Help me understand the code below:\"\n",
        "# print(user_query)\n",
        "file_path = \"/home/riddhi/Desktop/riddhi_workplace/Homework4/spark/examples/src/main/python/wordcount.py\"\n",
        "directory_path = \"/home/riddhi/Desktop/riddhi_workplace/Homework4/spark/examples/src/main/python\"\n",
        "response = answer_query(user_query, file_path, directory_path)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e59ee1-6d3a-41bf-87a7-d1df3146bb46",
      "metadata": {
        "id": "79e59ee1-6d3a-41bf-87a7-d1df3146bb46",
        "outputId": "9f85e68b-95ec-4921-c365-2601f94fe5f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cebc3d-5006-47f1-b3a6-eac6d79ebf95",
      "metadata": {
        "id": "94cebc3d-5006-47f1-b3a6-eac6d79ebf95",
        "outputId": "fe7ddb69-7783-4a6b-d7f3-b65ec42f0edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU esprima esprima tree_sitter tree_sitter_languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2980e30b-92ec-4854-92a2-7546a67ecea0",
      "metadata": {
        "id": "2980e30b-92ec-4854-92a2-7546a67ecea0",
        "outputId": "520bea13-b20d-443e-ffc6-2bb17bffc4b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "import sys\n",
            "from typing import Tuple\n",
            "\n",
            "from pyspark.rdd import RDD\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 2:\n",
            "        print(\"Usage: sort <file>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonSort\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "    sortedCount: RDD[Tuple[int, int]] = lines.flatMap(lambda x: x.split(' ')) \\\n",
            "        .map(lambda x: (int(x), 1)) \\\n",
            "        .sortByKey()\n",
            "    # This is just a demo on how to bring all the sorted data back to a single node.\n",
            "    # In reality, we wouldn't want to collect all the data to the driver node.\n",
            "    output = sortedCount.collect()\n",
            "    for (num, unitcount) in output:\n",
            "        print(num)\n",
            "\n",
            "    spark.stop()\n"
          ]
        }
      ],
      "source": [
        "# This is 2.3\n",
        "# Reference: https://python.langchain.com/docs/use_cases/code_understanding\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pprint import pprint\n",
        "from langchain_community.document_loaders.generic import GenericLoader\n",
        "from langchain_community.document_loaders.parsers import LanguageParser\n",
        "from langchain_text_splitters import Language\n",
        "\n",
        "# Load files that contain python code from repo to loader\n",
        "loader = GenericLoader.from_filesystem(\n",
        "    \"/home/riddhi/Desktop/riddhi_workplace/Homework4/spark/examples/src/main/python\",\n",
        "    glob=\"*\",\n",
        "    suffixes=[\".py\"],\n",
        "    parser=LanguageParser(language=Language.PYTHON),\n",
        ")\n",
        "docs = loader.load()\n",
        "# len(docs) # length of docs (No. of .py files)\n",
        "\n",
        "print(docs[0].page_content)\n",
        "# Print content from all files\n",
        "# print(\"\\n\\n--8<--\\n\\n\".join([document.page_content for document in docs]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f17a349-545d-41f1-974c-944177af15f6",
      "metadata": {
        "id": "3f17a349-545d-41f1-974c-944177af15f6",
        "outputId": "868e61d2-68d7-410f-dc13-8cd980f6a73b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet langchain-text-splitters tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2925af89-041c-4dcb-a1b1-e30918d2697c",
      "metadata": {
        "id": "2925af89-041c-4dcb-a1b1-e30918d2697c",
        "outputId": "aefc03e8-7ab7-4a6c-da51-e4a0376b9920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "import sys\n",
            "from typing import Tuple\n",
            "\n",
            "from pyspark.rdd import RDD\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "--8<--\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 2:\n",
            "        print(\"Usage: sort <file>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonSort\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "--8<--\n",
            "\n",
            "lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "    sortedCount: RDD[Tuple[int, int]] = lines.flatMap(lambda x: x.split(' ')) \\\n",
            "        .map(lambda x: (int(x), 1)) \\\n",
            "\n",
            "--8<--\n",
            "\n",
            ".sortByKey()\n",
            "    # This is just a demo on how to bring all the sorted data back to a single node.\n",
            "    # In reality, we wouldn't want to collect all the data to the driver node.\n",
            "\n",
            "--8<--\n",
            "\n",
            "output = sortedCount.collect()\n",
            "    for (num, unitcount) in output:\n",
            "        print(num)\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "import sys\n",
            "from operator import add\n",
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "--8<--\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 2:\n",
            "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonWordCount\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "--8<--\n",
            "\n",
            "lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "    counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
            "                  .map(lambda x: (x, 1)) \\\n",
            "                  .reduceByKey(add)\n",
            "\n",
            "--8<--\n",
            "\n",
            "output = counts.collect()\n",
            "    for (word, count) in output:\n",
            "        print(\"%s: %i\" % (word, count))\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "def generateGraph() -> Set[Tuple[int, int]]:\n",
            "    edges: Set[Tuple[int, int]] = set()\n",
            "    while len(edges) < numEdges:\n",
            "        src = rand.randrange(0, numVertices)\n",
            "\n",
            "--8<--\n",
            "\n",
            "dst = rand.randrange(0, numVertices)\n",
            "        if src != dst:\n",
            "            edges.add((src, dst))\n",
            "    return edges\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "import sys\n",
            "from random import Random\n",
            "from typing import Set, Tuple\n",
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "numEdges = 200\n",
            "numVertices = 100\n",
            "rand = Random(42)\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Code for: def generateGraph() -> Set[Tuple[int, int]]:\n",
            "\n",
            "--8<--\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    \"\"\"\n",
            "    Usage: transitive_closure [partitions]\n",
            "    \"\"\"\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonTransitiveClosure\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "--8<--\n",
            "\n",
            "partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
            "    tc = spark.sparkContext.parallelize(generateGraph(), partitions).cache()\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Linear transitive closure: each round grows paths by one edge,\n",
            "    # by joining the graph's edges with the already-discovered paths.\n",
            "\n",
            "--8<--\n",
            "\n",
            "# e.g. join the path (y, z) from the TC with the edge (x, y) from\n",
            "    # the graph to obtain the path (x, z).\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Because join() joins on keys, the edges are stored in reversed order.\n",
            "    edges = tc.map(lambda x_y: (x_y[1], x_y[0]))\n",
            "\n",
            "--8<--\n",
            "\n",
            "oldCount = 0\n",
            "    nextCount = tc.count()\n",
            "    while True:\n",
            "        oldCount = nextCount\n",
            "        # Perform the join, obtaining an RDD of (y, (z, x)) pairs,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# then project the result to obtain the new (x, z) paths.\n",
            "        new_edges = tc.join(edges).map(lambda __a_b: (__a_b[1][1], __a_b[1][0]))\n",
            "        tc = tc.union(new_edges).distinct().cache()\n",
            "\n",
            "--8<--\n",
            "\n",
            "nextCount = tc.count()\n",
            "        if nextCount == oldCount:\n",
            "            break\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"TC has %i edges\" % tc.count())\n",
            "\n",
            "    spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "\"\"\"\n",
            "Read data file users.parquet in local Spark distro:\n",
            "\n",
            "--8<--\n",
            "\n",
            "$ cd $SPARK_HOME\n",
            "$ export AVRO_PARQUET_JARS=/path/to/parquet-avro-1.5.0.jar\n",
            "$ ./bin/spark-submit --driver-class-path /path/to/example/jar \\\\\n",
            "        --jars $AVRO_PARQUET_JARS \\\\\n",
            "\n",
            "--8<--\n",
            "\n",
            "./examples/src/main/python/parquet_inputformat.py \\\\\n",
            "        examples/src/main/resources/users.parquet\n",
            "<...lots of log output...>\n",
            "\n",
            "--8<--\n",
            "\n",
            "{u'favorite_color': None, u'name': u'Alyssa', u'favorite_numbers': [3, 9, 15, 20]}\n",
            "{u'favorite_color': u'red', u'name': u'Ben', u'favorite_numbers': []}\n",
            "<...more log output...>\n",
            "\"\"\"\n",
            "import sys\n",
            "\n",
            "--8<--\n",
            "\n",
            "from typing import Any, Tuple\n",
            "\n",
            "--8<--\n",
            "\n",
            "from pyspark.rdd import RDD\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 2:\n",
            "        print(\"\"\"\n",
            "        Usage: parquet_inputformat.py <data_file>\n",
            "\n",
            "--8<--\n",
            "\n",
            "Run with example jar:\n",
            "        ./bin/spark-submit --driver-class-path /path/to/example/jar \\\\\n",
            "                /path/to/examples/parquet_inputformat.py <data_file>\n",
            "\n",
            "--8<--\n",
            "\n",
            "Assumes you have Parquet data stored in <data_file>.\n",
            "        \"\"\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "path = sys.argv[1]\n",
            "\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"ParquetInputFormat\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "    sc = spark.sparkContext\n",
            "\n",
            "--8<--\n",
            "\n",
            "parquet_rdd: RDD[Tuple[None, Any]] = sc.newAPIHadoopFile(\n",
            "        path,\n",
            "        'org.apache.parquet.avro.AvroParquetInputFormat',\n",
            "        'java.lang.Void',\n",
            "\n",
            "--8<--\n",
            "\n",
            "'org.apache.avro.generic.IndexedRecord',\n",
            "        valueConverter='org.apache.spark.examples.pythonconverters.IndexedRecordToJavaConverter')\n",
            "\n",
            "--8<--\n",
            "\n",
            "output = parquet_rdd.map(lambda x: x[1]).collect()\n",
            "    for k in output:\n",
            "        print(k)\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "def readPointBatch(iterator: Iterable[str]) -> List[np.ndarray]:\n",
            "    strs = list(iterator)\n",
            "    matrix = np.zeros((len(strs), D + 1))\n",
            "    for i, s in enumerate(strs):\n",
            "\n",
            "--8<--\n",
            "\n",
            "matrix[i] = np.fromstring(s.replace(',', ' '), dtype=np.float32, sep=' ')\n",
            "    return [matrix]\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "\"\"\"\n",
            "A logistic regression implementation that uses NumPy (http://www.numpy.org)\n",
            "to act on batches of input data using efficient matrix operations.\n",
            "\n",
            "--8<--\n",
            "\n",
            "In practice, one may prefer to use the LogisticRegression algorithm in\n",
            "ML, as shown in examples/src/main/python/ml/logistic_regression_with_elastic_net.py.\n",
            "\"\"\"\n",
            "import sys\n",
            "\n",
            "--8<--\n",
            "\n",
            "from typing import Iterable, List\n",
            "\n",
            "--8<--\n",
            "\n",
            "import numpy as np\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "\n",
            "D = 10  # Number of dimensions\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Read a batch of points from the input file into a NumPy matrix object. We operate on batches to\n",
            "# make further computations faster.\n",
            "\n",
            "--8<--\n",
            "\n",
            "# The data file contains lines of the form <label> <x1> <x2> ... <xD>. We load each block of these\n",
            "# into a NumPy array of size numLines * (D + 1) and pull out column 0 vs the others in gradient().\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Code for: def readPointBatch(iterator: Iterable[str]) -> List[np.ndarray]:\n",
            "\n",
            "--8<--\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "\n",
            "    if len(sys.argv) != 3:\n",
            "        print(\"Usage: logistic_regression <file> <iterations>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"\"\"WARN: This is a naive implementation of Logistic Regression and is\n",
            "      given as an example!\n",
            "      Please refer to examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "\n",
            "--8<--\n",
            "\n",
            "to see how ML's implementation is used.\"\"\", file=sys.stderr)\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonLR\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "--8<--\n",
            "\n",
            "points = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\\\n",
            "        .mapPartitions(readPointBatch).cache()\n",
            "    iterations = int(sys.argv[2])\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Initialize w to a random value\n",
            "    w = 2 * np.random.ranf(size=D) - 1\n",
            "    print(\"Initial w: \" + str(w))\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Compute logistic regression gradient for a matrix of data points\n",
            "    def gradient(matrix: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
            "\n",
            "--8<--\n",
            "\n",
            "Y = matrix[:, 0]    # point labels (first column of input file)\n",
            "        X = matrix[:, 1:]   # point coordinates\n",
            "        # For each point (x, y), compute gradient function, then sum these up\n",
            "\n",
            "--8<--\n",
            "\n",
            "return ((1.0 / (1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "def add(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
            "        x += y\n",
            "        return x\n",
            "\n",
            "--8<--\n",
            "\n",
            "for i in range(iterations):\n",
            "        print(\"On iteration %i\" % (i + 1))\n",
            "        w -= points.map(lambda m: gradient(m, w)).reduce(add)\n",
            "\n",
            "    print(\"Final w: \" + str(w))\n",
            "\n",
            "    spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "def rmse(R: np.ndarray, ms: np.ndarray, us: np.ndarray) -> np.float64:\n",
            "    diff = R - ms * us.T\n",
            "    return np.sqrt(np.sum(np.power(diff, 2)) / (M * U))\n",
            "\n",
            "--8<--\n",
            "\n",
            "def update(i: int, mat: np.ndarray, ratings: np.ndarray) -> np.ndarray:\n",
            "    uu = mat.shape[0]\n",
            "    ff = mat.shape[1]\n",
            "\n",
            "    XtX = mat.T * mat\n",
            "    Xty = mat.T * ratings[i, :].T\n",
            "\n",
            "--8<--\n",
            "\n",
            "for j in range(ff):\n",
            "        XtX[j, j] += LAMBDA * uu\n",
            "\n",
            "    return np.linalg.solve(XtX, Xty)\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "\"\"\"\n",
            "This is an example implementation of ALS for learning how to use Spark. Please refer to\n",
            "pyspark.ml.recommendation.ALS for more conventional use.\n",
            "\n",
            "--8<--\n",
            "\n",
            "This example requires numpy (http://www.numpy.org/)\n",
            "\"\"\"\n",
            "import sys\n",
            "\n",
            "import numpy as np\n",
            "from numpy.random import rand\n",
            "from numpy import matrix\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "--8<--\n",
            "\n",
            "LAMBDA = 0.01   # regularization\n",
            "np.random.seed(42)\n",
            "\n",
            "\n",
            "# Code for: def rmse(R: np.ndarray, ms: np.ndarray, us: np.ndarray) -> np.float64:\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Code for: def update(i: int, mat: np.ndarray, ratings: np.ndarray) -> np.ndarray:\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "\n",
            "    \"\"\"\n",
            "    Usage: als [M] [U] [F] [iterations] [partitions]\"\n",
            "    \"\"\"\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"\"\"WARN: This is a naive implementation of ALS and is given as an\n",
            "      example. Please use pyspark.ml.recommendation.ALS for more\n",
            "      conventional use.\"\"\", file=sys.stderr)\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonALS\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "    sc = spark.sparkContext\n",
            "\n",
            "--8<--\n",
            "\n",
            "M = int(sys.argv[1]) if len(sys.argv) > 1 else 100\n",
            "    U = int(sys.argv[2]) if len(sys.argv) > 2 else 500\n",
            "    F = int(sys.argv[3]) if len(sys.argv) > 3 else 10\n",
            "\n",
            "--8<--\n",
            "\n",
            "ITERATIONS = int(sys.argv[4]) if len(sys.argv) > 4 else 5\n",
            "    partitions = int(sys.argv[5]) if len(sys.argv) > 5 else 2\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"Running ALS with M=%d, U=%d, F=%d, iters=%d, partitions=%d\\n\" %\n",
            "          (M, U, F, ITERATIONS, partitions))\n",
            "\n",
            "--8<--\n",
            "\n",
            "R = matrix(rand(M, F)) * matrix(rand(U, F).T)\n",
            "    ms: matrix = matrix(rand(M, F))\n",
            "    us: matrix = matrix(rand(U, F))\n",
            "\n",
            "--8<--\n",
            "\n",
            "Rb = sc.broadcast(R)\n",
            "    msb = sc.broadcast(ms)\n",
            "    usb = sc.broadcast(us)\n",
            "\n",
            "--8<--\n",
            "\n",
            "for i in range(ITERATIONS):\n",
            "        ms_ = sc.parallelize(range(M), partitions) \\\n",
            "            .map(lambda x: update(x, usb.value, Rb.value)) \\\n",
            "            .collect()\n",
            "\n",
            "--8<--\n",
            "\n",
            "# collect() returns a list, so array ends up being\n",
            "        # a 3-d array, we take the first 2 dims for the matrix\n",
            "        ms = matrix(np.array(ms_)[:, :, 0])\n",
            "        msb = sc.broadcast(ms)\n",
            "\n",
            "--8<--\n",
            "\n",
            "us_ = sc.parallelize(range(U), partitions) \\\n",
            "            .map(lambda x: update(x, msb.value, Rb.value.T)) \\\n",
            "            .collect()\n",
            "        us = matrix(np.array(us_)[:, :, 0])\n",
            "\n",
            "--8<--\n",
            "\n",
            "usb = sc.broadcast(us)\n",
            "\n",
            "--8<--\n",
            "\n",
            "error = rmse(R, ms, us)\n",
            "        print(\"Iteration %d:\" % i)\n",
            "        print(\"\\nRMSE: %5.4f\\n\" % error)\n",
            "\n",
            "    spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "def parseVector(line: str) -> np.ndarray:\n",
            "    return np.array([float(x) for x in line.split(' ')])\n",
            "\n",
            "--8<--\n",
            "\n",
            "def closestPoint(p: np.ndarray, centers: List[np.ndarray]) -> int:\n",
            "    bestIndex = 0\n",
            "    closest = float(\"+inf\")\n",
            "    for i in range(len(centers)):\n",
            "        tempDist = np.sum((p - centers[i]) ** 2)\n",
            "\n",
            "--8<--\n",
            "\n",
            "if tempDist < closest:\n",
            "            closest = tempDist\n",
            "            bestIndex = i\n",
            "    return bestIndex\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "\"\"\"\n",
            "The K-means algorithm written from scratch against PySpark. In practice,\n",
            "one may prefer to use the KMeans algorithm in ML, as shown in\n",
            "examples/src/main/python/ml/kmeans_example.py.\n",
            "\n",
            "--8<--\n",
            "\n",
            "This example requires NumPy (http://www.numpy.org/).\n",
            "\"\"\"\n",
            "import sys\n",
            "from typing import List\n",
            "\n",
            "import numpy as np\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Code for: def parseVector(line: str) -> np.ndarray:\n",
            "\n",
            "\n",
            "# Code for: def closestPoint(p: np.ndarray, centers: List[np.ndarray]) -> int:\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "\n",
            "--8<--\n",
            "\n",
            "if len(sys.argv) != 4:\n",
            "        print(\"Usage: kmeans <file> <k> <convergeDist>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"\"\"WARN: This is a naive implementation of KMeans Clustering and is given\n",
            "       as an example! Please refer to examples/src/main/python/ml/kmeans_example.py for an\n",
            "\n",
            "--8<--\n",
            "\n",
            "example on how to use ML's KMeans implementation.\"\"\", file=sys.stderr)\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonKMeans\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "--8<--\n",
            "\n",
            "lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "    data = lines.map(parseVector).cache()\n",
            "    K = int(sys.argv[2])\n",
            "    convergeDist = float(sys.argv[3])\n",
            "\n",
            "--8<--\n",
            "\n",
            "kPoints = data.takeSample(False, K, 1)\n",
            "    tempDist = 1.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "while tempDist > convergeDist:\n",
            "        closest = data.map(\n",
            "            lambda p: (closestPoint(p, kPoints), (p, 1)))\n",
            "        pointStats = closest.reduceByKey(\n",
            "\n",
            "--8<--\n",
            "\n",
            "lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))\n",
            "        newPoints = pointStats.map(\n",
            "            lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
            "\n",
            "--8<--\n",
            "\n",
            "tempDist = sum(np.sum((kPoints[iK] - p) ** 2) for (iK, p) in newPoints)\n",
            "\n",
            "        for (iK, p) in newPoints:\n",
            "            kPoints[iK] = p\n",
            "\n",
            "    print(\"Final centers: \" + str(kPoints))\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "def computeContribs(urls: ResultIterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
            "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
            "    num_urls = len(urls)\n",
            "\n",
            "--8<--\n",
            "\n",
            "for url in urls:\n",
            "        yield (url, rank / num_urls)\n",
            "\n",
            "--8<--\n",
            "\n",
            "def parseNeighbors(urls: str) -> Tuple[str, str]:\n",
            "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
            "    parts = re.split(r'\\s+', urls)\n",
            "    return parts[0], parts[1]\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "\"\"\"\n",
            "This is an example implementation of PageRank. For more conventional use,\n",
            "Please refer to PageRank implementation provided by graphx\n",
            "\n",
            "--8<--\n",
            "\n",
            "Example Usage:\n",
            "bin/spark-submit examples/src/main/python/pagerank.py data/mllib/pagerank_data.txt 10\n",
            "\"\"\"\n",
            "import re\n",
            "import sys\n",
            "from operator import add\n",
            "from typing import Iterable, Tuple\n",
            "\n",
            "--8<--\n",
            "\n",
            "from pyspark.resultiterable import ResultIterable\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "\n",
            "# Code for: def computeContribs(urls: ResultIterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Code for: def parseNeighbors(urls: str) -> Tuple[str, str]:\n",
            "\n",
            "--8<--\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 3:\n",
            "        print(\"Usage: pagerank <file> <iterations>\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"WARN: This is a naive implementation of PageRank and is given as an example!\\n\" +\n",
            "          \"Please refer to PageRank implementation provided by graphx\",\n",
            "          file=sys.stderr)\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Initialize the spark context.\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonPageRank\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Loads in input file. It should be in format of:\n",
            "    #     URL         neighbor URL\n",
            "    #     URL         neighbor URL\n",
            "    #     URL         neighbor URL\n",
            "    #     ...\n",
            "\n",
            "--8<--\n",
            "\n",
            "lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Loads all URLs from input file and initialize their neighbors.\n",
            "    links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
            "    ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
            "    for iteration in range(int(sys.argv[2])):\n",
            "        # Calculates URL contributions to the rank of other URLs.\n",
            "\n",
            "--8<--\n",
            "\n",
            "contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(\n",
            "            url_urls_rank[1][0], url_urls_rank[1][1]  # type: ignore[arg-type]\n",
            "        ))\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Re-calculates URL ranks based on neighbor contributions.\n",
            "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Collects all URL ranks and dump them to console.\n",
            "    for (link, rank) in ranks.collect():\n",
            "        print(\"%s has rank: %s.\" % (link, rank))\n",
            "\n",
            "    spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "def delayed(seconds: int) -> Callable[[Any], Any]:\n",
            "    def f(x: int) -> int:\n",
            "        time.sleep(seconds)\n",
            "        return x\n",
            "    return f\n",
            "\n",
            "--8<--\n",
            "\n",
            "def call_in_background(f: Callable[..., Any], *args: Any) -> Queue.Queue:\n",
            "    result: Queue.Queue = Queue.Queue(1)\n",
            "    t = threading.Thread(target=lambda: result.put(f(*args)))\n",
            "    t.daemon = True\n",
            "\n",
            "--8<--\n",
            "\n",
            "t.start()\n",
            "    return result\n",
            "\n",
            "--8<--\n",
            "\n",
            "def main() -> None:\n",
            "    conf = SparkConf().set(\"spark.ui.showConsoleProgress\", \"false\")\n",
            "    sc = SparkContext(appName=\"PythonStatusAPIDemo\", conf=conf)\n",
            "\n",
            "--8<--\n",
            "\n",
            "def run() -> List[Tuple[int, int]]:\n",
            "        rdd = sc.parallelize(range(10), 10).map(delayed(2))\n",
            "        reduced = rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
            "\n",
            "--8<--\n",
            "\n",
            "return reduced.map(delayed(2)).collect()\n",
            "\n",
            "--8<--\n",
            "\n",
            "result = call_in_background(run)\n",
            "    status = sc.statusTracker()\n",
            "    while result.empty():\n",
            "        ids = status.getJobIdsForGroup()\n",
            "        for id in ids:\n",
            "            job = status.getJobInfo(id)\n",
            "\n",
            "--8<--\n",
            "\n",
            "assert job is not None\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"Job\", id, \"status: \", job.status)\n",
            "            for sid in job.stageIds:\n",
            "                info = status.getStageInfo(sid)\n",
            "                if info:\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"Stage %d: %d tasks total (%d active, %d complete)\" %\n",
            "                          (sid, info.numTasks, info.numActiveTasks, info.numCompletedTasks))\n",
            "        time.sleep(1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "print(\"Job results are:\", result.get())\n",
            "    sc.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "import time\n",
            "import threading\n",
            "import queue as Queue\n",
            "from typing import Any, Callable, List, Tuple\n",
            "\n",
            "from pyspark import SparkConf, SparkContext\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Code for: def delayed(seconds: int) -> Callable[[Any], Any]:\n",
            "\n",
            "\n",
            "# Code for: def call_in_background(f: Callable[..., Any], *args: Any) -> Queue.Queue:\n",
            "\n",
            "\n",
            "# Code for: def main() -> None:\n",
            "\n",
            "--8<--\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "\"\"\"\n",
            "Read data file users.avro in local Spark distro:\n",
            "\n",
            "--8<--\n",
            "\n",
            "$ cd $SPARK_HOME\n",
            "$ ./bin/spark-submit --driver-class-path /path/to/example/jar \\\n",
            "> ./examples/src/main/python/avro_inputformat.py \\\n",
            "> examples/src/main/resources/users.avro\n",
            "\n",
            "--8<--\n",
            "\n",
            "{u'favorite_color': None, u'name': u'Alyssa', u'favorite_numbers': [3, 9, 15, 20]}\n",
            "{u'favorite_color': u'red', u'name': u'Ben', u'favorite_numbers': []}\n",
            "\n",
            "--8<--\n",
            "\n",
            "To read name and favorite_color fields only, specify the following reader schema:\n",
            "\n",
            "--8<--\n",
            "\n",
            "$ cat examples/src/main/resources/user.avsc\n",
            "{\"namespace\": \"example.avro\",\n",
            " \"type\": \"record\",\n",
            " \"name\": \"User\",\n",
            " \"fields\": [\n",
            "     {\"name\": \"name\", \"type\": \"string\"},\n",
            "\n",
            "--8<--\n",
            "\n",
            "{\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
            " ]\n",
            "}\n",
            "\n",
            "--8<--\n",
            "\n",
            "$ ./bin/spark-submit --driver-class-path /path/to/example/jar \\\n",
            "> ./examples/src/main/python/avro_inputformat.py \\\n",
            "> examples/src/main/resources/users.avro examples/src/main/resources/user.avsc\n",
            "\n",
            "--8<--\n",
            "\n",
            "{u'favorite_color': None, u'name': u'Alyssa'}\n",
            "{u'favorite_color': u'red', u'name': u'Ben'}\n",
            "\"\"\"\n",
            "import sys\n",
            "from typing import Any, Tuple\n",
            "\n",
            "--8<--\n",
            "\n",
            "from functools import reduce\n",
            "from pyspark.rdd import RDD\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "--8<--\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    if len(sys.argv) != 2 and len(sys.argv) != 3:\n",
            "        print(\"\"\"\n",
            "        Usage: avro_inputformat <data_file> [reader_schema_file]\n",
            "\n",
            "--8<--\n",
            "\n",
            "Run with example jar:\n",
            "        ./bin/spark-submit --driver-class-path /path/to/example/jar \\\n",
            "        /path/to/examples/avro_inputformat.py <data_file> [reader_schema_file]\n",
            "\n",
            "--8<--\n",
            "\n",
            "Assumes you have Avro data stored in <data_file>. Reader schema can be optionally specified\n",
            "        in [reader_schema_file].\n",
            "        \"\"\", file=sys.stderr)\n",
            "        sys.exit(-1)\n",
            "\n",
            "--8<--\n",
            "\n",
            "path = sys.argv[1]\n",
            "\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"AvroKeyInputFormat\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "    sc = spark.sparkContext\n",
            "\n",
            "--8<--\n",
            "\n",
            "conf = None\n",
            "    if len(sys.argv) == 3:\n",
            "        schema_rdd = sc.textFile(sys.argv[2], 1).collect()\n",
            "        conf = {\"avro.schema.input.key\": reduce(lambda x, y: x + y, schema_rdd)}\n",
            "\n",
            "--8<--\n",
            "\n",
            "avro_rdd: RDD[Tuple[Any, None]] = sc.newAPIHadoopFile(\n",
            "        path,\n",
            "        \"org.apache.avro.mapreduce.AvroKeyInputFormat\",\n",
            "        \"org.apache.avro.mapred.AvroKey\",\n",
            "\n",
            "--8<--\n",
            "\n",
            "\"org.apache.hadoop.io.NullWritable\",\n",
            "        keyConverter=\"org.apache.spark.examples.pythonconverters.AvroWrapperToJavaConverter\",\n",
            "        conf=conf)\n",
            "\n",
            "--8<--\n",
            "\n",
            "output = avro_rdd.map(lambda x: x[0]).collect()\n",
            "    for k in output:\n",
            "        print(k)\n",
            "\n",
            "--8<--\n",
            "\n",
            "spark.stop()\n",
            "\n",
            "--8<--\n",
            "\n",
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
            "# contributor license agreements.  See the NOTICE file distributed with\n",
            "\n",
            "--8<--\n",
            "\n",
            "# this work for additional information regarding copyright ownership.\n",
            "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
            "\n",
            "--8<--\n",
            "\n",
            "# (the \"License\"); you may not use this file except in compliance with\n",
            "# the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#    http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "\n",
            "--8<--\n",
            "\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\n",
            "--8<--\n",
            "\n",
            "import sys\n",
            "from random import random\n",
            "from operator import add\n",
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "--8<--\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    \"\"\"\n",
            "        Usage: pi [partitions]\n",
            "    \"\"\"\n",
            "    spark = SparkSession\\\n",
            "        .builder\\\n",
            "        .appName(\"PythonPi\")\\\n",
            "        .getOrCreate()\n",
            "\n",
            "--8<--\n",
            "\n",
            "partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
            "    n = 100000 * partitions\n",
            "\n",
            "--8<--\n",
            "\n",
            "def f(_: int) -> float:\n",
            "        x = random() * 2 - 1\n",
            "        y = random() * 2 - 1\n",
            "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
            "\n",
            "--8<--\n",
            "\n",
            "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
            "    print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
            "\n",
            "    spark.stop()\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=200, chunk_overlap=0\n",
        ")\n",
        "texts = python_splitter.split_documents(docs)\n",
        "# len(texts)\n",
        "print(\"\\n\\n--8<--\\n\\n\".join([document.page_content for document in texts]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c72749-2e87-48de-9623-0c07e7982d12",
      "metadata": {
        "id": "49c72749-2e87-48de-9623-0c07e7982d12",
        "outputId": "f52b01a9-0bd8-4bfb-f1ae-d52d5f3d2d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in /home/riddhi/.local/lib/python3.10/site-packages (3.7.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /home/riddhi/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/riddhi/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/riddhi/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/riddhi/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/riddhi/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/riddhi/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/riddhi/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2a2158-ef9f-40d1-ad4c-ffa27af6182c",
      "metadata": {
        "id": "3c2a2158-ef9f-40d1-ad4c-ffa27af6182c",
        "outputId": "e0c0d3fe-452b-4dde-841c-92e5c5b7f70d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/riddhi/.local/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (59.6.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: jinja2 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/riddhi/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/riddhi/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/riddhi/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /home/riddhi/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2020.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/riddhi/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.5)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/riddhi/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/riddhi/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/riddhi/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451f6012-2531-4b79-838c-9dcd1ef29d85",
      "metadata": {
        "id": "451f6012-2531-4b79-838c-9dcd1ef29d85",
        "outputId": "7310abbd-70da-4de7-ae24-5bc5d1085ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "sys\n",
            " PERSON\n",
            "Tuple ORG\n",
            "RDD ORG\n",
            "2 CARDINAL\n",
            "int(x ORG\n",
            "1 CARDINAL\n",
            "\\ PERSON\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "sys\n",
            " PERSON\n",
            "2 CARDINAL\n",
            "\\\n",
            "                   PERSON\n",
            "1 CARDINAL\n",
            "\\ PERSON\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "rand.randrange(0 ORG\n",
            "numVertices ORG\n",
            "rand.randrange(0 ORG\n",
            "numVertices ORG\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "sys\n",
            " PERSON\n",
            "200 CARDINAL\n",
            "100 CARDINAL\n",
            "1 CARDINAL\n",
            "2 CARDINAL\n",
            "partitions).cache PERSON\n",
            "Linear ORG\n",
            "one CARDINAL\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "x_y[0] PERSON\n",
            "0 CARDINAL\n",
            "RDD ORG\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "Spark ORG\n",
            "$ cd $ MONEY\n",
            "./bin PERSON\n",
            "AVRO_PARQUET_JARS MONEY\n",
            "3 CARDINAL\n",
            "9 DATE\n",
            "15 DATE\n",
            "20 CARDINAL\n",
            "RDD ORG\n",
            "2 CARDINAL\n",
            "Parquet ORG\n",
            "org.apache.parquet.avro PRODUCT\n",
            "IndexedRecord ORG\n",
            "list(iterator ORG\n",
            "np.zeros((len(strs PERSON\n",
            "enumerate(strs GPE\n",
            "np.fromstring(s.replace PERSON\n",
            "np.float32 CARDINAL\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "one CARDINAL\n",
            "LogisticRegression ORG\n",
            "ML ORG\n",
            "Iterable, List WORK_OF_ART\n",
            "10 CARDINAL\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "numLines PRODUCT\n",
            "0 CARDINAL\n",
            "3 CARDINAL\n",
            "ML ORG\n",
            "# CARDINAL\n",
            "Initialize ORG\n",
            "2 CARDINAL\n",
            "1 CARDINAL\n",
            "gradient(matrix ORG\n",
            "[: PERSON\n",
            "0 CARDINAL\n",
            "first ORDINAL\n",
            "[: PERSON\n",
            "1:] QUANTITY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "1.0 CARDINAL\n",
            "print(\"On GPE\n",
            "gradient(m ORG\n",
            "np.sqrt(np.sum(np.power(diff ORG\n",
            "2 CARDINAL\n",
            "range(ff GPE\n",
            "Xty GPE\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "ALS ORG\n",
            "Spark GPE\n",
            "ALS ORG\n",
            "0.01 CARDINAL\n",
            "ALS ORG\n",
            "ALS ORG\n",
            "1 CARDINAL\n",
            "100 CARDINAL\n",
            "2 CARDINAL\n",
            "500 CARDINAL\n",
            "3 CARDINAL\n",
            "10 CARDINAL\n",
            "4 CARDINAL\n",
            "5 CARDINAL\n",
            "5 CARDINAL\n",
            "2 CARDINAL\n",
            "ALS ORG\n",
            "M=%d ORG\n",
            "U=%d PERSON\n",
            "F PRODUCT\n",
            "matrix(rand(M PERSON\n",
            "F ORG\n",
            "matrix(rand(U ORG\n",
            "matrix(rand(M PERSON\n",
            "F ORG\n",
            "matrix(rand(U ORG\n",
            "F ORG\n",
            "= PERSON\n",
            "usb.value ORG\n",
            "Rb.value ORG\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "3 CARDINAL\n",
            "first ORDINAL\n",
            "2 CARDINAL\n",
            "matrix(np.array(ms PERSON\n",
            "[: PERSON\n",
            "us GPE\n",
            "msb.value ORG\n",
            "Rb.value ORG\n",
            "matrix(np.array(us GPE\n",
            "[: PERSON\n",
            "us PERSON\n",
            "print(\"Iteration % PERCENT\n",
            "5.4f\\n\" % PERCENT\n",
            "0 CARDINAL\n",
            "2 CARDINAL\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "The K-means WORK_OF_ART\n",
            "PySpark ORG\n",
            "KMeans NORP\n",
            "ML GPE\n",
            "sys\n",
            " PERSON\n",
            "# Code MONEY\n",
            "4 CARDINAL\n",
            "kmeans NORP\n",
            "KMeans NORP\n",
            "ML ORG\n",
            "KMeans NORP\n",
            "K ORG\n",
            "1 CARDINAL\n",
            "1.0 CARDINAL\n",
            "closestPoint(p, kPoints GPE\n",
            "p1_c1[1] + PERSON\n",
            "2 CARDINAL\n",
            "newPoints GPE\n",
            "newPoints ORG\n",
            "re.split(r'\\s+ ORG\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "PageRank ORG\n",
            "PageRank ORG\n",
            "pagerank_data.txt ORG\n",
            "10 CARDINAL\n",
            "sys\n",
            " PERSON\n",
            "ResultIterable PRODUCT\n",
            "3 CARDINAL\n",
            "PageRank ORG\n",
            "PageRank ORG\n",
            "# CARDINAL\n",
            "#     URL          MONEY\n",
            "links.map(lambda CARDINAL\n",
            "1.0 CARDINAL\n",
            "PageRank ORG\n",
            "# CARDINAL\n",
            "0.85 + 0.15 DATE\n",
            "f(x ORG\n",
            "call_in_background(f WORK_OF_ART\n",
            "Queue(1 PERSON\n",
            "10).map(delayed(2 CARDINAL\n",
            "1)).reduceByKey(lambda CARDINAL\n",
            "status.getJobInfo(id ORG\n",
            "assert job PERSON\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "Queue PRODUCT\n",
            "SparkConf PRODUCT\n",
            "SparkContext ORG\n",
            "call_in_background(f WORK_OF_ART\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "Spark ORG\n",
            "$ cd $ MONEY\n",
            "./bin PERSON\n",
            "\\ PERSON\n",
            "3 CARDINAL\n",
            "9 DATE\n",
            "15 DATE\n",
            "20 CARDINAL\n",
            "./bin PERSON\n",
            "\\ PERSON\n",
            "sys\n",
            " PERSON\n",
            "RDD ORG\n",
            "2 CARDINAL\n",
            "3 CARDINAL\n",
            "Avro ORG\n",
            "3 CARDINAL\n",
            "1).collect DATE\n",
            "RDD[Tuple[Any CARDINAL\n",
            "AvroKey ORG\n",
            "NullWritable ORG\n",
            "# CARDINAL\n",
            "Licensed PERSON\n",
            "the Apache Software Foundation ORG\n",
            "ASF ORG\n",
            "# CARDINAL\n",
            "ASF ORG\n",
            "the Apache License ORG\n",
            "2.0 CARDINAL\n",
            "License WORK_OF_ART\n",
            "# the License MONEY\n",
            "# CARDINAL\n",
            "# CARDINAL\n",
            "License ORG\n",
            "# CARDINAL\n",
            "License ORG\n",
            "sys\n",
            " PERSON\n",
            "1 CARDINAL\n",
            "2 CARDINAL\n",
            "100000 DATE\n",
            "def f PERSON\n",
            "2 CARDINAL\n",
            "2 CARDINAL\n",
            "1 CARDINAL\n",
            "2 + DATE\n",
            "2 CARDINAL\n",
            "1 CARDINAL\n",
            "0 CARDINAL\n",
            "roughly % PERCENT\n",
            "4.0 CARDINAL\n"
          ]
        }
      ],
      "source": [
        "# Name Entity recognition\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Iterate over each document in the list\n",
        "for document in texts:\n",
        "    # Get the text content from the document\n",
        "    text = document.page_content\n",
        "\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Iterate over the entities found in the text\n",
        "    for ent in doc.ents:\n",
        "        print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9165a182-a502-4802-9483-c9bd86c86019",
      "metadata": {
        "id": "9165a182-a502-4802-9483-c9bd86c86019",
        "outputId": "fae4252b-cad3-4b34-fffd-bcd68ab08b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from mrjob.job import MRJob\n",
            "import re\n",
            "\n",
            "class AverageRating(MRJob):\n",
            "    # Parses each line of input data and extracts the product name and calculated \n",
            "    # unique word count for customer reviews. The input data is in CSV format with columns for date, \n",
            "    # rating, review, and product. This filters out the header row, splits the \n",
            "    # CSV data, and calculates the word count for review text using the get_word_count()\n",
            "    # method before yielding the product name and score.\n",
            "    def mapper(self, _, line):\n",
            "        if line.lower().startswith('date'):  # Skip header row\n",
            "            return\n",
            "\n",
            "        # Input data is in CSV format with columns: date, rating, review, product\n",
            "        data = line.strip().split('|')\n",
            "        product = data[2]\n",
            "        word_count = self.get_word_count(data[3])\n",
            "        yield product, word_count\n",
            "\n",
            "    def reducer(self, product, word_count):\n",
            "        word_count_list = list(word_count) \n",
            "        total_word_count = sum(word_count_list)\n",
            "        total_reviews = len(word_count_list)\n",
            "        average_words_per_review = round(total_word_count / total_reviews) if total_reviews > 0 else 0\n",
            "        yield product, average_words_per_review\n",
            "\n",
            "    def clean_text(self, text):\n",
            "        text = re.sub(r'\\W', ' ', text) \n",
            "        text = re.sub(r'\\s+', ' ', text)  \n",
            "        text = re.sub(r'-\\s*', '', text)\n",
            "        return text.lower()\n",
            "\n",
            "    def get_word_count(self, text):\n",
            "        refined_text = self.clean_text(text)\n",
            "        unique_words = {word:1 for word in refined_text.split(\" \")}\n",
            "        return len(unique_words.keys())\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    AverageRating.run()\n",
            "\n",
            "**Explanation of the Code:**\n",
            "\n",
            "The code you provided is a MapReduce job that calculates the average number of unique words in customer reviews for each product. It uses the mrjob library, which is a Python framework for writing Hadoop-style MapReduce jobs.\n",
            "\n",
            "**1. Importing Libraries:**\n",
            "\n",
            "```python\n",
            "from mrjob.job import MRJob\n",
            "import re\n",
            "```\n",
            "\n",
            "* `from mrjob.job import MRJob`: Imports the `MRJob` class from the mrjob library.\n",
            "* `import re`: Imports the `re` module for regular expression manipulation.\n",
            "\n",
            "**2. Defining the `AverageRating` Class:**\n",
            "\n",
            "```python\n",
            "class AverageRating(MRJob):\n",
            "```\n",
            "\n",
            "This class defines the MapReduce job. It inherits from the `MRJob` class provided by the mrjob library.\n",
            "\n",
            "**3. Mapper Function:**\n",
            "\n",
            "```python\n",
            "def mapper(self, _, line):\n",
            "```\n",
            "\n",
            "* The mapper function is defined as `mapper(self, _, line)`. It takes two arguments:\n",
            "    * `self`: Reference to the current job instance.\n",
            "    * `_`: A placeholder variable used to discard the key of the input data (which is not used).\n",
            "    * `line`: A single line of input data.\n",
            "\n",
            "* The mapper function does the following:\n",
            "    * Skips the header row of the input data, which starts with \"Date\".\n",
            "    * Splits the CSV data into a list `data`.\n",
            "    * Extracts the product name and review text from the data.\n",
            "    * Calculates the number of unique words in the review text using the `get_word_count()` method.\n",
            "    * Yields the product name as the key and the word count as the value.\n",
            "\n",
            "**4. Reducer Function:**\n",
            "\n",
            "```python\n",
            "def reducer(self, product, word_count):\n",
            "```\n",
            "\n",
            "* The reducer function is defined as `reducer(self, product, word_count)`. It takes two arguments:\n",
            "    * `self`: Reference to the current job instance.\n",
            "    * `product`: The product name.\n",
            "    * `word_count`: An iterable containing the word counts for the product.\n",
            "\n",
            "* The reducer function does the following:\n",
            "    * Creates a list `word_count_list` from the `word_count` iterable.\n",
            "    * Calculates the total word count and total number of reviews.\n",
            "    * Calculates the average number of unique words per review for the product.\n",
            "    * Yields the product name as the key and the average word count as the value.\n",
            "\n",
            "**5. Helper Functions:**\n",
            "\n",
            "```python\n",
            "def clean_text(self, text):\n",
            "```\n",
            "\n",
            "* The `clean_text()` function is a helper function used to clean the review text by removing non-alphanumeric characters, extra spaces, and dashes.\n",
            "\n",
            "```python\n",
            "def get_word_count(self, text):\n",
            "```\n",
            "\n",
            "* The `get_word_count()` function is a helper function used to calculate the number of unique words in the review text. It splits the text into words, removes duplicate words, and returns the count of unique words.\n",
            "\n",
            "**6. Main Function:**\n",
            "\n",
            "```python\n",
            "if __name__ == '__main__':\n",
            "    AverageRating.run()\n",
            "```\n",
            "\n",
            "* The main function checks if the script is being run as the main program (rather than imported as a module).\n",
            "* If so, it calls the `run()` method of the `AverageRating` class, which executes the MapReduce job.\n",
            "\n",
            "Thank you for using our service. If you have any further questions, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# Previous Homework analysis - avg_content_length.py\n",
        "# Example usage of answer_query function\n",
        "hw_query = \"Explain the code:\"\n",
        "# print(user_query)\n",
        "file_path = \"/home/riddhi/Desktop/riddhi_workplace/Homework4/avg_content_length.py\"\n",
        "hw_response = answer_query(hw_query, file_path, directory_path)\n",
        "print(hw_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b5701c-0d1f-4e23-b863-73ca84a189f2",
      "metadata": {
        "id": "f3b5701c-0d1f-4e23-b863-73ca84a189f2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}